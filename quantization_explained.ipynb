{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 神经网络量化技术详解 🔥\n",
        "\n",
        "## 概述\n",
        "本notebook详细解释神经网络量化的工作原理，特别是为什么8GB GPU需要使用4-bit量化来运行大型语言模型。\n",
        "\n",
        "### 量化的核心概念：\n",
        "- **量化**：将高精度浮点数转换为低精度整数\n",
        "- **目标**：大幅减少内存占用，使大模型能在有限硬件上运行\n",
        "- **权衡**：内存节省 vs 精度损失\n",
        "\n",
        "### 学习目标：\n",
        "1. 理解不同精度的内存占用差异\n",
        "2. 掌握量化和反量化的数学原理\n",
        "3. 实际体验量化对模型性能的影响\n",
        "4. 了解如何选择合适的量化策略\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 环境设置完成!\n",
            "PyTorch版本: 2.7.1+cu126\n",
            "CUDA是否可用: True\n",
            "GPU: NVIDIA GeForce GTX 1080\n",
            "GPU显存: 8.0 GB\n"
          ]
        }
      ],
      "source": [
        "# 导入必要的库\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# 设置绘图风格\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# 设置随机种子以确保结果可重现\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"🚀 环境设置完成!\")\n",
        "print(f\"PyTorch版本: {torch.__version__}\")\n",
        "print(f\"CUDA是否可用: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU显存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. 模拟神经网络权重与内存占用分析 📊\n",
        "\n",
        "我们首先创建一个模拟的神经网络权重矩阵，然后分析不同精度下的内存占用情况。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建一个模拟的权重矩阵 (假设是某一层的权重)\n",
        "original_weights = torch.randn(1000, 1000) * 2  # 1M个参数\n",
        "print(f\"📐 原始权重矩阵形状: {original_weights.shape}\")\n",
        "print(f\"📈 原始权重范围: [{original_weights.min():.3f}, {original_weights.max():.3f}]\")\n",
        "print(f\"🔢 总参数数量: {original_weights.numel():,}\")\n",
        "\n",
        "def calculate_memory_usage(tensor, dtype):\n",
        "    \"\"\"计算张量在不同数据类型下的内存占用\"\"\"\n",
        "    bytes_per_element = {\n",
        "        'fp32': 4,    # 32-bit = 4 bytes\n",
        "        'fp16': 2,    # 16-bit = 2 bytes\n",
        "        'int8': 1,    # 8-bit = 1 byte\n",
        "        'int4': 0.5   # 4-bit = 0.5 bytes\n",
        "    }\n",
        "\n",
        "    total_bytes = tensor.numel() * bytes_per_element[dtype]\n",
        "    return total_bytes / (1024**2)  # 转换为MB\n",
        "\n",
        "# 计算不同精度的内存占用\n",
        "memory_usage = {}\n",
        "dtypes = ['fp32', 'fp16', 'int8', 'int4']\n",
        "\n",
        "print(\"\\n💾 内存占用对比:\")\n",
        "print(\"-\" * 40)\n",
        "for dtype in dtypes:\n",
        "    memory_mb = calculate_memory_usage(original_weights, dtype)\n",
        "    memory_usage[dtype] = memory_mb\n",
        "    compression_ratio = memory_usage['fp32'] / memory_mb\n",
        "    print(f\"{dtype.upper():>5}: {memory_mb:>7.2f} MB (压缩比: {compression_ratio:.1f}x)\")\n",
        "\n",
        "# 对于7B参数模型的实际内存需求\n",
        "print(f\"\\n🧠 对于7B参数模型的实际内存需求:\")\n",
        "print(\"-\" * 50)\n",
        "model_params = 7e9  # 7 billion parameters\n",
        "\n",
        "for dtype in dtypes:\n",
        "    bytes_per_param = {'fp32': 4, 'fp16': 2, 'int8': 1, 'int4': 0.5}[dtype]\n",
        "    total_gb = (model_params * bytes_per_param) / (1024**3)\n",
        "    print(f\"{dtype.upper():>5}: {total_gb:>6.1f} GB\")\n",
        "\n",
        "print(f\"\\n💡 你的GTX 1080显存: 8.0 GB\")\n",
        "print(f\"✅ 结论: 需要4-bit量化才能装载7B模型!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. 量化算法实现 🔧\n",
        "\n",
        "现在我们实现8-bit和4-bit量化算法，了解量化的数学原理。\n",
        "\n",
        "### 量化公式：\n",
        "- **量化**: `quantized_value = round((original_value - zero_point) / scale)`\n",
        "- **反量化**: `reconstructed_value = quantized_value * scale + zero_point`\n",
        "\n",
        "其中：\n",
        "- `scale = (max_val - min_val) / (2^bits - 1)`\n",
        "- `zero_point = min_val`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8-bit量化实现\n",
        "def quantize_to_int8(tensor):\n",
        "    \"\"\"将FP32张量量化为INT8\"\"\"\n",
        "    # 计算量化参数\n",
        "    min_val = tensor.min()\n",
        "    max_val = tensor.max()\n",
        "\n",
        "    # 计算缩放因子和零点 (INT8范围: 0-255)\n",
        "    scale = (max_val - min_val) / 255.0\n",
        "    zero_point = min_val\n",
        "\n",
        "    print(f\"8-bit量化参数:\")\n",
        "    print(f\"  原始范围: [{min_val:.4f}, {max_val:.4f}]\")\n",
        "    print(f\"  缩放因子: {scale:.6f}\")\n",
        "    print(f\"  零点: {zero_point:.4f}\")\n",
        "\n",
        "    # 量化: (原值 - 零点) / 缩放因子\n",
        "    quantized = torch.round((tensor - zero_point) / scale).clamp(0, 255).to(torch.uint8)\n",
        "\n",
        "    return quantized, scale, zero_point\n",
        "\n",
        "def dequantize_from_int8(quantized_tensor, scale, zero_point):\n",
        "    \"\"\"将INT8张量反量化为FP32\"\"\"\n",
        "    # 反量化: 量化值 * 缩放因子 + 零点\n",
        "    return quantized_tensor.float() * scale + zero_point\n",
        "\n",
        "# 4-bit量化实现\n",
        "def quantize_to_int4(tensor):\n",
        "    \"\"\"将FP32张量量化为INT4 (0-15范围)\"\"\"\n",
        "    min_val = tensor.min()\n",
        "    max_val = tensor.max()\n",
        "\n",
        "    # INT4范围是0-15\n",
        "    scale = (max_val - min_val) / 15.0\n",
        "    zero_point = min_val\n",
        "\n",
        "    print(f\"\\n4-bit量化参数:\")\n",
        "    print(f\"  原始范围: [{min_val:.4f}, {max_val:.4f}]\")\n",
        "    print(f\"  缩放因子: {scale:.6f}\")\n",
        "    print(f\"  零点: {zero_point:.4f}\")\n",
        "\n",
        "    quantized = torch.round((tensor - zero_point) / scale).clamp(0, 15).to(torch.uint8)\n",
        "\n",
        "    return quantized, scale, zero_point\n",
        "\n",
        "def dequantize_from_int4(quantized_tensor, scale, zero_point):\n",
        "    \"\"\"将INT4张量反量化为FP32\"\"\"\n",
        "    return quantized_tensor.float() * scale + zero_point\n",
        "\n",
        "print(\"✅ 量化函数定义完成!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. 量化实验 🔬\n",
        "\n",
        "让我们对模拟的权重矩阵进行量化实验，观察量化前后的数值变化。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 执行量化实验\n",
        "print(\"🔬 开始量化实验...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 8-bit量化\n",
        "weights_int8, scale_8bit, zero_point_8bit = quantize_to_int8(original_weights)\n",
        "weights_dequant_8bit = dequantize_from_int8(weights_int8, scale_8bit, zero_point_8bit)\n",
        "\n",
        "# 4-bit量化\n",
        "weights_int4, scale_4bit, zero_point_4bit = quantize_to_int4(original_weights)\n",
        "weights_dequant_4bit = dequantize_from_int4(weights_int4, scale_4bit, zero_point_4bit)\n",
        "\n",
        "print(f\"\\n📊 权重样本对比 (前5个元素):\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"原始权重:    {original_weights[0, :5]}\")\n",
        "print(f\"8-bit量化:   {weights_int8[0, :5]}\")\n",
        "print(f\"8-bit反量化: {weights_dequant_8bit[0, :5]}\")\n",
        "print(f\"4-bit量化:   {weights_int4[0, :5]}\")\n",
        "print(f\"4-bit反量化: {weights_dequant_4bit[0, :5]}\")\n",
        "\n",
        "# 计算量化误差\n",
        "def calculate_quantization_error(original, quantized):\n",
        "    \"\"\"计算量化误差的各种指标\"\"\"\n",
        "    mse = torch.mean((original - quantized) ** 2)\n",
        "    mae = torch.mean(torch.abs(original - quantized))\n",
        "    max_error = torch.max(torch.abs(original - quantized))\n",
        "\n",
        "    # 计算信噪比 (SNR)\n",
        "    signal_power = torch.mean(original ** 2)\n",
        "    noise_power = mse\n",
        "    snr_db = 10 * torch.log10(signal_power / noise_power)\n",
        "\n",
        "    return mse.item(), mae.item(), max_error.item(), snr_db.item()\n",
        "\n",
        "mse_8bit, mae_8bit, max_error_8bit, snr_8bit = calculate_quantization_error(original_weights, weights_dequant_8bit)\n",
        "mse_4bit, mae_4bit, max_error_4bit, snr_4bit = calculate_quantization_error(original_weights, weights_dequant_4bit)\n",
        "\n",
        "print(f\"\\n📈 精度损失分析:\")\n",
        "print(\"-\" * 60)\n",
        "print(\"8-bit量化:\")\n",
        "print(f\"  均方误差(MSE):     {mse_8bit:.8f}\")\n",
        "print(f\"  平均绝对误差(MAE): {mae_8bit:.8f}\")\n",
        "print(f\"  最大误差:          {max_error_8bit:.8f}\")\n",
        "print(f\"  信噪比(SNR):       {snr_8bit:.2f} dB\")\n",
        "\n",
        "print(\"\\n4-bit量化:\")\n",
        "print(f\"  均方误差(MSE):     {mse_4bit:.8f}\")\n",
        "print(f\"  平均绝对误差(MAE): {mae_4bit:.8f}\")\n",
        "print(f\"  最大误差:          {max_error_4bit:.8f}\")\n",
        "print(f\"  信噪比(SNR):       {snr_4bit:.2f} dB\")\n",
        "\n",
        "print(f\"\\n💡 量化质量评估:\")\n",
        "print(f\"  8-bit量化误差相对较小，适合高精度需求\")\n",
        "print(f\"  4-bit量化误差较大，但对于大模型推理通常可接受\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. 模拟神经网络前向传播 🧠\n",
        "\n",
        "现在我们模拟实际的神经网络计算，看看量化对最终输出的影响。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 模拟神经网络前向传播\n",
        "print(\"🧠 模拟神经网络前向传播...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 创建模拟输入数据 (batch_size=32, input_dim=1000)\n",
        "input_data = torch.randn(32, 1000)\n",
        "print(f\"输入数据形状: {input_data.shape}\")\n",
        "\n",
        "def forward_pass(weights, input_data, precision_name):\n",
        "    \"\"\"模拟前向传播: output = input @ weights.T\"\"\"\n",
        "    output = torch.matmul(input_data, weights.T)\n",
        "    return output\n",
        "\n",
        "# 使用不同精度的权重进行计算\n",
        "print(f\"\\n🔄 使用不同精度权重进行前向传播...\")\n",
        "\n",
        "output_original = forward_pass(original_weights, input_data, \"FP32原始\")\n",
        "output_8bit = forward_pass(weights_dequant_8bit, input_data, \"8-bit量化\")\n",
        "output_4bit = forward_pass(weights_dequant_4bit, input_data, \"4-bit量化\")\n",
        "\n",
        "print(f\"原始输出形状: {output_original.shape}\")\n",
        "print(f\"输出数值范围: [{output_original.min():.3f}, {output_original.max():.3f}]\")\n",
        "\n",
        "# 计算输出差异\n",
        "def calculate_output_difference(original, quantized, name):\n",
        "    \"\"\"计算量化后输出与原始输出的差异\"\"\"\n",
        "    abs_diff = torch.abs(original - quantized)\n",
        "    relative_diff = abs_diff / (torch.abs(original) + 1e-8)  # 避免除零\n",
        "\n",
        "    mean_abs_diff = torch.mean(abs_diff)\n",
        "    max_abs_diff = torch.max(abs_diff)\n",
        "    mean_rel_diff = torch.mean(relative_diff) * 100  # 转换为百分比\n",
        "\n",
        "    print(f\"\\n{name}输出差异:\")\n",
        "    print(f\"  平均绝对差异: {mean_abs_diff:.6f}\")\n",
        "    print(f\"  最大绝对差异: {max_abs_diff:.6f}\")\n",
        "    print(f\"  平均相对差异: {mean_rel_diff:.3f}%\")\n",
        "\n",
        "    return mean_abs_diff.item(), max_abs_diff.item(), mean_rel_diff.item()\n",
        "\n",
        "diff_8bit = calculate_output_difference(output_original, output_8bit, \"8-bit\")\n",
        "diff_4bit = calculate_output_difference(output_original, output_4bit, \"4-bit\")\n",
        "\n",
        "# 模拟多层网络的累积误差\n",
        "print(f\"\\n🔗 模拟多层网络的累积误差效应:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "num_layers = 5\n",
        "current_output_orig = input_data\n",
        "current_output_8bit = input_data\n",
        "current_output_4bit = input_data\n",
        "\n",
        "cumulative_errors_8bit = []\n",
        "cumulative_errors_4bit = []\n",
        "\n",
        "for layer in range(num_layers):\n",
        "    # 为每一层创建新的权重矩阵\n",
        "    layer_weights = torch.randn(1000, 1000) * 0.1  # 较小的权重\n",
        "\n",
        "    # 量化权重\n",
        "    w8, s8, z8 = quantize_to_int8(layer_weights)\n",
        "    w8_dequant = dequantize_from_int8(w8, s8, z8)\n",
        "\n",
        "    w4, s4, z4 = quantize_to_int4(layer_weights)\n",
        "    w4_dequant = dequantize_from_int4(w4, s4, z4)\n",
        "\n",
        "    # 前向传播\n",
        "    current_output_orig = torch.matmul(current_output_orig, layer_weights.T)\n",
        "    current_output_8bit = torch.matmul(current_output_8bit, w8_dequant.T)\n",
        "    current_output_4bit = torch.matmul(current_output_4bit, w4_dequant.T)\n",
        "\n",
        "    # 计算累积误差\n",
        "    error_8bit = torch.mean(torch.abs(current_output_orig - current_output_8bit))\n",
        "    error_4bit = torch.mean(torch.abs(current_output_orig - current_output_4bit))\n",
        "\n",
        "    cumulative_errors_8bit.append(error_8bit.item())\n",
        "    cumulative_errors_4bit.append(error_4bit.item())\n",
        "\n",
        "    print(f\"Layer {layer+1}: 8-bit误差={error_8bit:.6f}, 4-bit误差={error_4bit:.6f}\")\n",
        "\n",
        "print(f\"\\n📊 观察: 误差随网络深度逐渐累积，但4-bit量化在实际应用中仍然可用\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. 可视化分析 📊\n",
        "\n",
        "现在让我们通过图表直观地展示量化的效果。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建综合可视化图表\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('神经网络量化技术全面分析', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. 权重分布对比\n",
        "ax1 = axes[0, 0]\n",
        "sample_size = 10000\n",
        "orig_sample = original_weights.flatten()[:sample_size].numpy()\n",
        "w8_sample = weights_dequant_8bit.flatten()[:sample_size].numpy()\n",
        "w4_sample = weights_dequant_4bit.flatten()[:sample_size].numpy()\n",
        "\n",
        "ax1.hist(orig_sample, bins=50, alpha=0.6, label='原始 FP32', color='blue', density=True)\n",
        "ax1.hist(w8_sample, bins=50, alpha=0.6, label='8-bit 反量化', color='orange', density=True)\n",
        "ax1.hist(w4_sample, bins=50, alpha=0.6, label='4-bit 反量化', color='red', density=True)\n",
        "ax1.set_xlabel('权重值')\n",
        "ax1.set_ylabel('概率密度')\n",
        "ax1.set_title('权重分布对比')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. 量化误差分布\n",
        "ax2 = axes[0, 1]\n",
        "error_8bit = (original_weights - weights_dequant_8bit).flatten()[:sample_size].numpy()\n",
        "error_4bit = (original_weights - weights_dequant_4bit).flatten()[:sample_size].numpy()\n",
        "\n",
        "ax2.hist(error_8bit, bins=50, alpha=0.7, label='8-bit误差', color='orange', density=True)\n",
        "ax2.hist(error_4bit, bins=50, alpha=0.7, label='4-bit误差', color='red', density=True)\n",
        "ax2.set_xlabel('量化误差')\n",
        "ax2.set_ylabel('概率密度')\n",
        "ax2.set_title('量化误差分布')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. 内存占用对比\n",
        "ax3 = axes[0, 2]\n",
        "memory_fp32 = calculate_memory_usage(original_weights, 'fp32')\n",
        "memory_fp16 = calculate_memory_usage(original_weights, 'fp16')\n",
        "memory_int8 = calculate_memory_usage(original_weights, 'int8')\n",
        "memory_int4 = calculate_memory_usage(original_weights, 'int4')\n",
        "\n",
        "memory_values = [memory_fp32, memory_fp16, memory_int8, memory_int4]\n",
        "labels = ['FP32', 'FP16', 'INT8', 'INT4']\n",
        "colors = ['red', 'orange', 'green', 'blue']\n",
        "\n",
        "bars = ax3.bar(labels, memory_values, color=colors, alpha=0.7)\n",
        "ax3.set_ylabel('内存占用 (MB)')\n",
        "ax3.set_title('不同精度的内存占用')\n",
        "ax3.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "# 在柱状图上添加数值标签\n",
        "for bar, value in zip(bars, memory_values):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "             f'{value:.1f}MB', ha='center', va='bottom')\n",
        "\n",
        "# 4. 量化前后权重对比 (样本)\n",
        "ax4 = axes[1, 0]\n",
        "sample_indices = range(100)\n",
        "sample_orig = original_weights[0, :100].numpy()\n",
        "sample_8bit = weights_dequant_8bit[0, :100].numpy()\n",
        "sample_4bit = weights_dequant_4bit[0, :100].numpy()\n",
        "\n",
        "ax4.plot(sample_indices, sample_orig, 'b-', label='原始权重', linewidth=2, alpha=0.8)\n",
        "ax4.plot(sample_indices, sample_8bit, 'o--', label='8-bit量化', markersize=3, alpha=0.7)\n",
        "ax4.plot(sample_indices, sample_4bit, 's--', label='4-bit量化', markersize=3, alpha=0.7)\n",
        "ax4.set_xlabel('权重索引')\n",
        "ax4.set_ylabel('权重值')\n",
        "ax4.set_title('量化前后权重对比 (前100个)')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. 累积误差随网络深度变化\n",
        "ax5 = axes[1, 1]\n",
        "layers = range(1, len(cumulative_errors_8bit) + 1)\n",
        "ax5.plot(layers, cumulative_errors_8bit, 'o-', label='8-bit累积误差', linewidth=2, markersize=6)\n",
        "ax5.plot(layers, cumulative_errors_4bit, 's-', label='4-bit累积误差', linewidth=2, markersize=6)\n",
        "ax5.set_xlabel('网络层数')\n",
        "ax5.set_ylabel('累积误差')\n",
        "ax5.set_title('多层网络累积误差')\n",
        "ax5.legend()\n",
        "ax5.grid(True, alpha=0.3)\n",
        "ax5.set_yscale('log')\n",
        "\n",
        "# 6. 7B模型在不同GPU上的适配性\n",
        "ax6 = axes[1, 2]\n",
        "gpu_memories = ['GTX 1080\\n(8GB)', 'RTX 3080\\n(10GB)', 'RTX 4090\\n(24GB)', 'A100\\n(40GB)']\n",
        "memory_limits = [8, 10, 24, 40]\n",
        "\n",
        "# 7B模型在不同精度下的内存需求\n",
        "model_7b_fp32 = 28\n",
        "model_7b_fp16 = 14\n",
        "model_7b_int8 = 7\n",
        "model_7b_int4 = 3.5\n",
        "\n",
        "x_pos = np.arange(len(gpu_memories))\n",
        "width = 0.2\n",
        "\n",
        "ax6.bar(x_pos - 1.5*width, [model_7b_fp32]*4, width, label='FP32', color='red', alpha=0.7)\n",
        "ax6.bar(x_pos - 0.5*width, [model_7b_fp16]*4, width, label='FP16', color='orange', alpha=0.7)\n",
        "ax6.bar(x_pos + 0.5*width, [model_7b_int8]*4, width, label='INT8', color='green', alpha=0.7)\n",
        "ax6.bar(x_pos + 1.5*width, [model_7b_int4]*4, width, label='INT4', color='blue', alpha=0.7)\n",
        "\n",
        "# 添加GPU内存限制线\n",
        "for i, limit in enumerate(memory_limits):\n",
        "    ax6.axhline(y=limit, xmin=(i-0.4)/len(gpu_memories), xmax=(i+0.4)/len(gpu_memories),\n",
        "                color='black', linestyle='--', linewidth=2)\n",
        "\n",
        "ax6.set_ylabel('内存需求 (GB)')\n",
        "ax6.set_title('7B模型在不同GPU上的适配性')\n",
        "ax6.set_xticks(x_pos)\n",
        "ax6.set_xticklabels(gpu_memories)\n",
        "ax6.legend()\n",
        "ax6.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 打印关键统计信息\n",
        "print(\"\\n📊 量化效果总结:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"内存压缩效果:\")\n",
        "print(f\"  8-bit量化: {memory_fp32/memory_int8:.1f}x 压缩\")\n",
        "print(f\"  4-bit量化: {memory_fp32/memory_int4:.1f}x 压缩\")\n",
        "print(f\"\\n精度保持:\")\n",
        "print(f\"  8-bit信噪比: {snr_8bit:.1f} dB\")\n",
        "print(f\"  4-bit信噪比: {snr_4bit:.1f} dB\")\n",
        "print(f\"\\n实际应用建议:\")\n",
        "print(f\"  8GB GPU: 使用4-bit量化 ✅\")\n",
        "print(f\"  16GB GPU: 使用8-bit量化 ✅\")\n",
        "print(f\"  24GB+ GPU: 可以使用FP16 ✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. 总结与实际应用 🎯\n",
        "\n",
        "### 量化技术的核心要点\n",
        "\n",
        "#### 🔥 量化本质\n",
        "- **核心原理**: 将高精度浮点数映射到低精度整数范围\n",
        "- **数学基础**: 线性量化公式 `quantized = (original - zero_point) / scale`\n",
        "- **反量化**: `reconstructed = quantized * scale + zero_point`\n",
        "\n",
        "#### 💾 内存节省效果\n",
        "| 精度类型 | 每参数字节数 | 7B模型内存需求 | 压缩比 |\n",
        "|---------|-------------|---------------|--------|\n",
        "| FP32    | 4 bytes     | 28 GB        | 1x     |\n",
        "| FP16    | 2 bytes     | 14 GB        | 2x     |\n",
        "| INT8    | 1 byte      | 7 GB         | 4x     |\n",
        "| INT4    | 0.5 bytes   | 3.5 GB       | 8x     |\n",
        "\n",
        "#### ⚖️ 精度权衡\n",
        "- **8-bit量化**: 误差很小，几乎无损\n",
        "- **4-bit量化**: 有一定误差，但实际应用中可接受\n",
        "- **误差累积**: 随网络深度增加，但影响有限\n",
        "\n",
        "### 🛠️ 实际应用指南\n",
        "\n",
        "#### GPU选择策略\n",
        "```python\n",
        "# 根据GPU显存选择量化策略\n",
        "if gpu_memory <= 8:\n",
        "    quantization = \"4-bit\"  # 必须\n",
        "elif gpu_memory <= 16:\n",
        "    quantization = \"8-bit\"  # 推荐\n",
        "else:\n",
        "    quantization = \"FP16\"   # 可选\n",
        "```\n",
        "\n",
        "#### 模型加载最佳实践\n",
        "```python\n",
        "# LLaVA模型加载示例\n",
        "model = load_pretrained_model(\n",
        "    model_path=\"liuhaotian/llava-v1.5-7b\",\n",
        "    load_4bit=True,          # 4-bit量化\n",
        "    device_map=\"auto\",       # 自动分配\n",
        ")\n",
        "```\n",
        "\n",
        "### 🚀 性能优化技巧\n",
        "\n",
        "1. **智能内存管理**: `device_map=\"auto\"` 自动分配GPU/CPU\n",
        "2. **混合精度**: 关键层使用高精度，其他层量化\n",
        "3. **动态加载**: 按需加载模型层，减少显存占用\n",
        "4. **梯度检查点**: 训练时节省显存\n",
        "\n",
        "### 🎓 学习收获\n",
        "\n",
        "通过本notebook，你应该理解了：\n",
        "- ✅ 为什么8GB GPU需要4-bit量化\n",
        "- ✅ 量化的数学原理和实现方法\n",
        "- ✅ 量化对模型性能的实际影响\n",
        "- ✅ 如何选择适合的量化策略\n",
        "\n",
        "### 🔗 扩展阅读\n",
        "\n",
        "- [Quantization and Training of Neural Networks](https://arxiv.org/abs/1712.05877)\n",
        "- [LLM.int8(): 8-bit Matrix Multiplication](https://arxiv.org/abs/2208.07339)\n",
        "- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
        "\n",
        "---\n",
        "\n",
        "**🎉 恭喜！你已经掌握了神经网络量化的核心概念！**\n",
        "\n",
        "现在你可以自信地在有限的硬件资源上运行大型语言模型了！ 🚀\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
