{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ç¥ç»ç½‘ç»œé‡åŒ–æŠ€æœ¯è¯¦è§£ ğŸ”¥\n",
        "\n",
        "## æ¦‚è¿°\n",
        "æœ¬notebookè¯¦ç»†è§£é‡Šç¥ç»ç½‘ç»œé‡åŒ–çš„å·¥ä½œåŸç†ï¼Œç‰¹åˆ«æ˜¯ä¸ºä»€ä¹ˆ8GB GPUéœ€è¦ä½¿ç”¨4-bité‡åŒ–æ¥è¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚\n",
        "\n",
        "### é‡åŒ–çš„æ ¸å¿ƒæ¦‚å¿µï¼š\n",
        "- **é‡åŒ–**ï¼šå°†é«˜ç²¾åº¦æµ®ç‚¹æ•°è½¬æ¢ä¸ºä½ç²¾åº¦æ•´æ•°\n",
        "- **ç›®æ ‡**ï¼šå¤§å¹…å‡å°‘å†…å­˜å ç”¨ï¼Œä½¿å¤§æ¨¡å‹èƒ½åœ¨æœ‰é™ç¡¬ä»¶ä¸Šè¿è¡Œ\n",
        "- **æƒè¡¡**ï¼šå†…å­˜èŠ‚çœ vs ç²¾åº¦æŸå¤±\n",
        "\n",
        "### å­¦ä¹ ç›®æ ‡ï¼š\n",
        "1. ç†è§£ä¸åŒç²¾åº¦çš„å†…å­˜å ç”¨å·®å¼‚\n",
        "2. æŒæ¡é‡åŒ–å’Œåé‡åŒ–çš„æ•°å­¦åŸç†\n",
        "3. å®é™…ä½“éªŒé‡åŒ–å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“\n",
        "4. äº†è§£å¦‚ä½•é€‰æ‹©åˆé€‚çš„é‡åŒ–ç­–ç•¥\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ ç¯å¢ƒè®¾ç½®å®Œæˆ!\n",
            "PyTorchç‰ˆæœ¬: 2.7.1+cu126\n",
            "CUDAæ˜¯å¦å¯ç”¨: True\n",
            "GPU: NVIDIA GeForce GTX 1080\n",
            "GPUæ˜¾å­˜: 8.0 GB\n"
          ]
        }
      ],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„åº“\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# è®¾ç½®ç»˜å›¾é£æ ¼\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"ğŸš€ ç¯å¢ƒè®¾ç½®å®Œæˆ!\")\n",
        "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
        "print(f\"CUDAæ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. æ¨¡æ‹Ÿç¥ç»ç½‘ç»œæƒé‡ä¸å†…å­˜å ç”¨åˆ†æ ğŸ“Š\n",
        "\n",
        "æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„ç¥ç»ç½‘ç»œæƒé‡çŸ©é˜µï¼Œç„¶ååˆ†æä¸åŒç²¾åº¦ä¸‹çš„å†…å­˜å ç”¨æƒ…å†µã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„æƒé‡çŸ©é˜µ (å‡è®¾æ˜¯æŸä¸€å±‚çš„æƒé‡)\n",
        "original_weights = torch.randn(1000, 1000) * 2  # 1Mä¸ªå‚æ•°\n",
        "print(f\"ğŸ“ åŸå§‹æƒé‡çŸ©é˜µå½¢çŠ¶: {original_weights.shape}\")\n",
        "print(f\"ğŸ“ˆ åŸå§‹æƒé‡èŒƒå›´: [{original_weights.min():.3f}, {original_weights.max():.3f}]\")\n",
        "print(f\"ğŸ”¢ æ€»å‚æ•°æ•°é‡: {original_weights.numel():,}\")\n",
        "\n",
        "def calculate_memory_usage(tensor, dtype):\n",
        "    \"\"\"è®¡ç®—å¼ é‡åœ¨ä¸åŒæ•°æ®ç±»å‹ä¸‹çš„å†…å­˜å ç”¨\"\"\"\n",
        "    bytes_per_element = {\n",
        "        'fp32': 4,    # 32-bit = 4 bytes\n",
        "        'fp16': 2,    # 16-bit = 2 bytes\n",
        "        'int8': 1,    # 8-bit = 1 byte\n",
        "        'int4': 0.5   # 4-bit = 0.5 bytes\n",
        "    }\n",
        "\n",
        "    total_bytes = tensor.numel() * bytes_per_element[dtype]\n",
        "    return total_bytes / (1024**2)  # è½¬æ¢ä¸ºMB\n",
        "\n",
        "# è®¡ç®—ä¸åŒç²¾åº¦çš„å†…å­˜å ç”¨\n",
        "memory_usage = {}\n",
        "dtypes = ['fp32', 'fp16', 'int8', 'int4']\n",
        "\n",
        "print(\"\\nğŸ’¾ å†…å­˜å ç”¨å¯¹æ¯”:\")\n",
        "print(\"-\" * 40)\n",
        "for dtype in dtypes:\n",
        "    memory_mb = calculate_memory_usage(original_weights, dtype)\n",
        "    memory_usage[dtype] = memory_mb\n",
        "    compression_ratio = memory_usage['fp32'] / memory_mb\n",
        "    print(f\"{dtype.upper():>5}: {memory_mb:>7.2f} MB (å‹ç¼©æ¯”: {compression_ratio:.1f}x)\")\n",
        "\n",
        "# å¯¹äº7Bå‚æ•°æ¨¡å‹çš„å®é™…å†…å­˜éœ€æ±‚\n",
        "print(f\"\\nğŸ§  å¯¹äº7Bå‚æ•°æ¨¡å‹çš„å®é™…å†…å­˜éœ€æ±‚:\")\n",
        "print(\"-\" * 50)\n",
        "model_params = 7e9  # 7 billion parameters\n",
        "\n",
        "for dtype in dtypes:\n",
        "    bytes_per_param = {'fp32': 4, 'fp16': 2, 'int8': 1, 'int4': 0.5}[dtype]\n",
        "    total_gb = (model_params * bytes_per_param) / (1024**3)\n",
        "    print(f\"{dtype.upper():>5}: {total_gb:>6.1f} GB\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ ä½ çš„GTX 1080æ˜¾å­˜: 8.0 GB\")\n",
        "print(f\"âœ… ç»“è®º: éœ€è¦4-bité‡åŒ–æ‰èƒ½è£…è½½7Bæ¨¡å‹!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. é‡åŒ–ç®—æ³•å®ç° ğŸ”§\n",
        "\n",
        "ç°åœ¨æˆ‘ä»¬å®ç°8-bitå’Œ4-bité‡åŒ–ç®—æ³•ï¼Œäº†è§£é‡åŒ–çš„æ•°å­¦åŸç†ã€‚\n",
        "\n",
        "### é‡åŒ–å…¬å¼ï¼š\n",
        "- **é‡åŒ–**: `quantized_value = round((original_value - zero_point) / scale)`\n",
        "- **åé‡åŒ–**: `reconstructed_value = quantized_value * scale + zero_point`\n",
        "\n",
        "å…¶ä¸­ï¼š\n",
        "- `scale = (max_val - min_val) / (2^bits - 1)`\n",
        "- `zero_point = min_val`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8-bité‡åŒ–å®ç°\n",
        "def quantize_to_int8(tensor):\n",
        "    \"\"\"å°†FP32å¼ é‡é‡åŒ–ä¸ºINT8\"\"\"\n",
        "    # è®¡ç®—é‡åŒ–å‚æ•°\n",
        "    min_val = tensor.min()\n",
        "    max_val = tensor.max()\n",
        "\n",
        "    # è®¡ç®—ç¼©æ”¾å› å­å’Œé›¶ç‚¹ (INT8èŒƒå›´: 0-255)\n",
        "    scale = (max_val - min_val) / 255.0\n",
        "    zero_point = min_val\n",
        "\n",
        "    print(f\"8-bité‡åŒ–å‚æ•°:\")\n",
        "    print(f\"  åŸå§‹èŒƒå›´: [{min_val:.4f}, {max_val:.4f}]\")\n",
        "    print(f\"  ç¼©æ”¾å› å­: {scale:.6f}\")\n",
        "    print(f\"  é›¶ç‚¹: {zero_point:.4f}\")\n",
        "\n",
        "    # é‡åŒ–: (åŸå€¼ - é›¶ç‚¹) / ç¼©æ”¾å› å­\n",
        "    quantized = torch.round((tensor - zero_point) / scale).clamp(0, 255).to(torch.uint8)\n",
        "\n",
        "    return quantized, scale, zero_point\n",
        "\n",
        "def dequantize_from_int8(quantized_tensor, scale, zero_point):\n",
        "    \"\"\"å°†INT8å¼ é‡åé‡åŒ–ä¸ºFP32\"\"\"\n",
        "    # åé‡åŒ–: é‡åŒ–å€¼ * ç¼©æ”¾å› å­ + é›¶ç‚¹\n",
        "    return quantized_tensor.float() * scale + zero_point\n",
        "\n",
        "# 4-bité‡åŒ–å®ç°\n",
        "def quantize_to_int4(tensor):\n",
        "    \"\"\"å°†FP32å¼ é‡é‡åŒ–ä¸ºINT4 (0-15èŒƒå›´)\"\"\"\n",
        "    min_val = tensor.min()\n",
        "    max_val = tensor.max()\n",
        "\n",
        "    # INT4èŒƒå›´æ˜¯0-15\n",
        "    scale = (max_val - min_val) / 15.0\n",
        "    zero_point = min_val\n",
        "\n",
        "    print(f\"\\n4-bité‡åŒ–å‚æ•°:\")\n",
        "    print(f\"  åŸå§‹èŒƒå›´: [{min_val:.4f}, {max_val:.4f}]\")\n",
        "    print(f\"  ç¼©æ”¾å› å­: {scale:.6f}\")\n",
        "    print(f\"  é›¶ç‚¹: {zero_point:.4f}\")\n",
        "\n",
        "    quantized = torch.round((tensor - zero_point) / scale).clamp(0, 15).to(torch.uint8)\n",
        "\n",
        "    return quantized, scale, zero_point\n",
        "\n",
        "def dequantize_from_int4(quantized_tensor, scale, zero_point):\n",
        "    \"\"\"å°†INT4å¼ é‡åé‡åŒ–ä¸ºFP32\"\"\"\n",
        "    return quantized_tensor.float() * scale + zero_point\n",
        "\n",
        "print(\"âœ… é‡åŒ–å‡½æ•°å®šä¹‰å®Œæˆ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. é‡åŒ–å®éªŒ ğŸ”¬\n",
        "\n",
        "è®©æˆ‘ä»¬å¯¹æ¨¡æ‹Ÿçš„æƒé‡çŸ©é˜µè¿›è¡Œé‡åŒ–å®éªŒï¼Œè§‚å¯Ÿé‡åŒ–å‰åçš„æ•°å€¼å˜åŒ–ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ‰§è¡Œé‡åŒ–å®éªŒ\n",
        "print(\"ğŸ”¬ å¼€å§‹é‡åŒ–å®éªŒ...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 8-bité‡åŒ–\n",
        "weights_int8, scale_8bit, zero_point_8bit = quantize_to_int8(original_weights)\n",
        "weights_dequant_8bit = dequantize_from_int8(weights_int8, scale_8bit, zero_point_8bit)\n",
        "\n",
        "# 4-bité‡åŒ–\n",
        "weights_int4, scale_4bit, zero_point_4bit = quantize_to_int4(original_weights)\n",
        "weights_dequant_4bit = dequantize_from_int4(weights_int4, scale_4bit, zero_point_4bit)\n",
        "\n",
        "print(f\"\\nğŸ“Š æƒé‡æ ·æœ¬å¯¹æ¯” (å‰5ä¸ªå…ƒç´ ):\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"åŸå§‹æƒé‡:    {original_weights[0, :5]}\")\n",
        "print(f\"8-bité‡åŒ–:   {weights_int8[0, :5]}\")\n",
        "print(f\"8-bitåé‡åŒ–: {weights_dequant_8bit[0, :5]}\")\n",
        "print(f\"4-bité‡åŒ–:   {weights_int4[0, :5]}\")\n",
        "print(f\"4-bitåé‡åŒ–: {weights_dequant_4bit[0, :5]}\")\n",
        "\n",
        "# è®¡ç®—é‡åŒ–è¯¯å·®\n",
        "def calculate_quantization_error(original, quantized):\n",
        "    \"\"\"è®¡ç®—é‡åŒ–è¯¯å·®çš„å„ç§æŒ‡æ ‡\"\"\"\n",
        "    mse = torch.mean((original - quantized) ** 2)\n",
        "    mae = torch.mean(torch.abs(original - quantized))\n",
        "    max_error = torch.max(torch.abs(original - quantized))\n",
        "\n",
        "    # è®¡ç®—ä¿¡å™ªæ¯” (SNR)\n",
        "    signal_power = torch.mean(original ** 2)\n",
        "    noise_power = mse\n",
        "    snr_db = 10 * torch.log10(signal_power / noise_power)\n",
        "\n",
        "    return mse.item(), mae.item(), max_error.item(), snr_db.item()\n",
        "\n",
        "mse_8bit, mae_8bit, max_error_8bit, snr_8bit = calculate_quantization_error(original_weights, weights_dequant_8bit)\n",
        "mse_4bit, mae_4bit, max_error_4bit, snr_4bit = calculate_quantization_error(original_weights, weights_dequant_4bit)\n",
        "\n",
        "print(f\"\\nğŸ“ˆ ç²¾åº¦æŸå¤±åˆ†æ:\")\n",
        "print(\"-\" * 60)\n",
        "print(\"8-bité‡åŒ–:\")\n",
        "print(f\"  å‡æ–¹è¯¯å·®(MSE):     {mse_8bit:.8f}\")\n",
        "print(f\"  å¹³å‡ç»å¯¹è¯¯å·®(MAE): {mae_8bit:.8f}\")\n",
        "print(f\"  æœ€å¤§è¯¯å·®:          {max_error_8bit:.8f}\")\n",
        "print(f\"  ä¿¡å™ªæ¯”(SNR):       {snr_8bit:.2f} dB\")\n",
        "\n",
        "print(\"\\n4-bité‡åŒ–:\")\n",
        "print(f\"  å‡æ–¹è¯¯å·®(MSE):     {mse_4bit:.8f}\")\n",
        "print(f\"  å¹³å‡ç»å¯¹è¯¯å·®(MAE): {mae_4bit:.8f}\")\n",
        "print(f\"  æœ€å¤§è¯¯å·®:          {max_error_4bit:.8f}\")\n",
        "print(f\"  ä¿¡å™ªæ¯”(SNR):       {snr_4bit:.2f} dB\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ é‡åŒ–è´¨é‡è¯„ä¼°:\")\n",
        "print(f\"  8-bité‡åŒ–è¯¯å·®ç›¸å¯¹è¾ƒå°ï¼Œé€‚åˆé«˜ç²¾åº¦éœ€æ±‚\")\n",
        "print(f\"  4-bité‡åŒ–è¯¯å·®è¾ƒå¤§ï¼Œä½†å¯¹äºå¤§æ¨¡å‹æ¨ç†é€šå¸¸å¯æ¥å—\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. æ¨¡æ‹Ÿç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­ ğŸ§ \n",
        "\n",
        "ç°åœ¨æˆ‘ä»¬æ¨¡æ‹Ÿå®é™…çš„ç¥ç»ç½‘ç»œè®¡ç®—ï¼Œçœ‹çœ‹é‡åŒ–å¯¹æœ€ç»ˆè¾“å‡ºçš„å½±å“ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ¨¡æ‹Ÿç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­\n",
        "print(\"ğŸ§  æ¨¡æ‹Ÿç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥æ•°æ® (batch_size=32, input_dim=1000)\n",
        "input_data = torch.randn(32, 1000)\n",
        "print(f\"è¾“å…¥æ•°æ®å½¢çŠ¶: {input_data.shape}\")\n",
        "\n",
        "def forward_pass(weights, input_data, precision_name):\n",
        "    \"\"\"æ¨¡æ‹Ÿå‰å‘ä¼ æ’­: output = input @ weights.T\"\"\"\n",
        "    output = torch.matmul(input_data, weights.T)\n",
        "    return output\n",
        "\n",
        "# ä½¿ç”¨ä¸åŒç²¾åº¦çš„æƒé‡è¿›è¡Œè®¡ç®—\n",
        "print(f\"\\nğŸ”„ ä½¿ç”¨ä¸åŒç²¾åº¦æƒé‡è¿›è¡Œå‰å‘ä¼ æ’­...\")\n",
        "\n",
        "output_original = forward_pass(original_weights, input_data, \"FP32åŸå§‹\")\n",
        "output_8bit = forward_pass(weights_dequant_8bit, input_data, \"8-bité‡åŒ–\")\n",
        "output_4bit = forward_pass(weights_dequant_4bit, input_data, \"4-bité‡åŒ–\")\n",
        "\n",
        "print(f\"åŸå§‹è¾“å‡ºå½¢çŠ¶: {output_original.shape}\")\n",
        "print(f\"è¾“å‡ºæ•°å€¼èŒƒå›´: [{output_original.min():.3f}, {output_original.max():.3f}]\")\n",
        "\n",
        "# è®¡ç®—è¾“å‡ºå·®å¼‚\n",
        "def calculate_output_difference(original, quantized, name):\n",
        "    \"\"\"è®¡ç®—é‡åŒ–åè¾“å‡ºä¸åŸå§‹è¾“å‡ºçš„å·®å¼‚\"\"\"\n",
        "    abs_diff = torch.abs(original - quantized)\n",
        "    relative_diff = abs_diff / (torch.abs(original) + 1e-8)  # é¿å…é™¤é›¶\n",
        "\n",
        "    mean_abs_diff = torch.mean(abs_diff)\n",
        "    max_abs_diff = torch.max(abs_diff)\n",
        "    mean_rel_diff = torch.mean(relative_diff) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”\n",
        "\n",
        "    print(f\"\\n{name}è¾“å‡ºå·®å¼‚:\")\n",
        "    print(f\"  å¹³å‡ç»å¯¹å·®å¼‚: {mean_abs_diff:.6f}\")\n",
        "    print(f\"  æœ€å¤§ç»å¯¹å·®å¼‚: {max_abs_diff:.6f}\")\n",
        "    print(f\"  å¹³å‡ç›¸å¯¹å·®å¼‚: {mean_rel_diff:.3f}%\")\n",
        "\n",
        "    return mean_abs_diff.item(), max_abs_diff.item(), mean_rel_diff.item()\n",
        "\n",
        "diff_8bit = calculate_output_difference(output_original, output_8bit, \"8-bit\")\n",
        "diff_4bit = calculate_output_difference(output_original, output_4bit, \"4-bit\")\n",
        "\n",
        "# æ¨¡æ‹Ÿå¤šå±‚ç½‘ç»œçš„ç´¯ç§¯è¯¯å·®\n",
        "print(f\"\\nğŸ”— æ¨¡æ‹Ÿå¤šå±‚ç½‘ç»œçš„ç´¯ç§¯è¯¯å·®æ•ˆåº”:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "num_layers = 5\n",
        "current_output_orig = input_data\n",
        "current_output_8bit = input_data\n",
        "current_output_4bit = input_data\n",
        "\n",
        "cumulative_errors_8bit = []\n",
        "cumulative_errors_4bit = []\n",
        "\n",
        "for layer in range(num_layers):\n",
        "    # ä¸ºæ¯ä¸€å±‚åˆ›å»ºæ–°çš„æƒé‡çŸ©é˜µ\n",
        "    layer_weights = torch.randn(1000, 1000) * 0.1  # è¾ƒå°çš„æƒé‡\n",
        "\n",
        "    # é‡åŒ–æƒé‡\n",
        "    w8, s8, z8 = quantize_to_int8(layer_weights)\n",
        "    w8_dequant = dequantize_from_int8(w8, s8, z8)\n",
        "\n",
        "    w4, s4, z4 = quantize_to_int4(layer_weights)\n",
        "    w4_dequant = dequantize_from_int4(w4, s4, z4)\n",
        "\n",
        "    # å‰å‘ä¼ æ’­\n",
        "    current_output_orig = torch.matmul(current_output_orig, layer_weights.T)\n",
        "    current_output_8bit = torch.matmul(current_output_8bit, w8_dequant.T)\n",
        "    current_output_4bit = torch.matmul(current_output_4bit, w4_dequant.T)\n",
        "\n",
        "    # è®¡ç®—ç´¯ç§¯è¯¯å·®\n",
        "    error_8bit = torch.mean(torch.abs(current_output_orig - current_output_8bit))\n",
        "    error_4bit = torch.mean(torch.abs(current_output_orig - current_output_4bit))\n",
        "\n",
        "    cumulative_errors_8bit.append(error_8bit.item())\n",
        "    cumulative_errors_4bit.append(error_4bit.item())\n",
        "\n",
        "    print(f\"Layer {layer+1}: 8-bitè¯¯å·®={error_8bit:.6f}, 4-bitè¯¯å·®={error_4bit:.6f}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š è§‚å¯Ÿ: è¯¯å·®éšç½‘ç»œæ·±åº¦é€æ¸ç´¯ç§¯ï¼Œä½†4-bité‡åŒ–åœ¨å®é™…åº”ç”¨ä¸­ä»ç„¶å¯ç”¨\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. å¯è§†åŒ–åˆ†æ ğŸ“Š\n",
        "\n",
        "ç°åœ¨è®©æˆ‘ä»¬é€šè¿‡å›¾è¡¨ç›´è§‚åœ°å±•ç¤ºé‡åŒ–çš„æ•ˆæœã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ›å»ºç»¼åˆå¯è§†åŒ–å›¾è¡¨\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('ç¥ç»ç½‘ç»œé‡åŒ–æŠ€æœ¯å…¨é¢åˆ†æ', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. æƒé‡åˆ†å¸ƒå¯¹æ¯”\n",
        "ax1 = axes[0, 0]\n",
        "sample_size = 10000\n",
        "orig_sample = original_weights.flatten()[:sample_size].numpy()\n",
        "w8_sample = weights_dequant_8bit.flatten()[:sample_size].numpy()\n",
        "w4_sample = weights_dequant_4bit.flatten()[:sample_size].numpy()\n",
        "\n",
        "ax1.hist(orig_sample, bins=50, alpha=0.6, label='åŸå§‹ FP32', color='blue', density=True)\n",
        "ax1.hist(w8_sample, bins=50, alpha=0.6, label='8-bit åé‡åŒ–', color='orange', density=True)\n",
        "ax1.hist(w4_sample, bins=50, alpha=0.6, label='4-bit åé‡åŒ–', color='red', density=True)\n",
        "ax1.set_xlabel('æƒé‡å€¼')\n",
        "ax1.set_ylabel('æ¦‚ç‡å¯†åº¦')\n",
        "ax1.set_title('æƒé‡åˆ†å¸ƒå¯¹æ¯”')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. é‡åŒ–è¯¯å·®åˆ†å¸ƒ\n",
        "ax2 = axes[0, 1]\n",
        "error_8bit = (original_weights - weights_dequant_8bit).flatten()[:sample_size].numpy()\n",
        "error_4bit = (original_weights - weights_dequant_4bit).flatten()[:sample_size].numpy()\n",
        "\n",
        "ax2.hist(error_8bit, bins=50, alpha=0.7, label='8-bitè¯¯å·®', color='orange', density=True)\n",
        "ax2.hist(error_4bit, bins=50, alpha=0.7, label='4-bitè¯¯å·®', color='red', density=True)\n",
        "ax2.set_xlabel('é‡åŒ–è¯¯å·®')\n",
        "ax2.set_ylabel('æ¦‚ç‡å¯†åº¦')\n",
        "ax2.set_title('é‡åŒ–è¯¯å·®åˆ†å¸ƒ')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. å†…å­˜å ç”¨å¯¹æ¯”\n",
        "ax3 = axes[0, 2]\n",
        "memory_fp32 = calculate_memory_usage(original_weights, 'fp32')\n",
        "memory_fp16 = calculate_memory_usage(original_weights, 'fp16')\n",
        "memory_int8 = calculate_memory_usage(original_weights, 'int8')\n",
        "memory_int4 = calculate_memory_usage(original_weights, 'int4')\n",
        "\n",
        "memory_values = [memory_fp32, memory_fp16, memory_int8, memory_int4]\n",
        "labels = ['FP32', 'FP16', 'INT8', 'INT4']\n",
        "colors = ['red', 'orange', 'green', 'blue']\n",
        "\n",
        "bars = ax3.bar(labels, memory_values, color=colors, alpha=0.7)\n",
        "ax3.set_ylabel('å†…å­˜å ç”¨ (MB)')\n",
        "ax3.set_title('ä¸åŒç²¾åº¦çš„å†…å­˜å ç”¨')\n",
        "ax3.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "# åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾\n",
        "for bar, value in zip(bars, memory_values):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "             f'{value:.1f}MB', ha='center', va='bottom')\n",
        "\n",
        "# 4. é‡åŒ–å‰åæƒé‡å¯¹æ¯” (æ ·æœ¬)\n",
        "ax4 = axes[1, 0]\n",
        "sample_indices = range(100)\n",
        "sample_orig = original_weights[0, :100].numpy()\n",
        "sample_8bit = weights_dequant_8bit[0, :100].numpy()\n",
        "sample_4bit = weights_dequant_4bit[0, :100].numpy()\n",
        "\n",
        "ax4.plot(sample_indices, sample_orig, 'b-', label='åŸå§‹æƒé‡', linewidth=2, alpha=0.8)\n",
        "ax4.plot(sample_indices, sample_8bit, 'o--', label='8-bité‡åŒ–', markersize=3, alpha=0.7)\n",
        "ax4.plot(sample_indices, sample_4bit, 's--', label='4-bité‡åŒ–', markersize=3, alpha=0.7)\n",
        "ax4.set_xlabel('æƒé‡ç´¢å¼•')\n",
        "ax4.set_ylabel('æƒé‡å€¼')\n",
        "ax4.set_title('é‡åŒ–å‰åæƒé‡å¯¹æ¯” (å‰100ä¸ª)')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. ç´¯ç§¯è¯¯å·®éšç½‘ç»œæ·±åº¦å˜åŒ–\n",
        "ax5 = axes[1, 1]\n",
        "layers = range(1, len(cumulative_errors_8bit) + 1)\n",
        "ax5.plot(layers, cumulative_errors_8bit, 'o-', label='8-bitç´¯ç§¯è¯¯å·®', linewidth=2, markersize=6)\n",
        "ax5.plot(layers, cumulative_errors_4bit, 's-', label='4-bitç´¯ç§¯è¯¯å·®', linewidth=2, markersize=6)\n",
        "ax5.set_xlabel('ç½‘ç»œå±‚æ•°')\n",
        "ax5.set_ylabel('ç´¯ç§¯è¯¯å·®')\n",
        "ax5.set_title('å¤šå±‚ç½‘ç»œç´¯ç§¯è¯¯å·®')\n",
        "ax5.legend()\n",
        "ax5.grid(True, alpha=0.3)\n",
        "ax5.set_yscale('log')\n",
        "\n",
        "# 6. 7Bæ¨¡å‹åœ¨ä¸åŒGPUä¸Šçš„é€‚é…æ€§\n",
        "ax6 = axes[1, 2]\n",
        "gpu_memories = ['GTX 1080\\n(8GB)', 'RTX 3080\\n(10GB)', 'RTX 4090\\n(24GB)', 'A100\\n(40GB)']\n",
        "memory_limits = [8, 10, 24, 40]\n",
        "\n",
        "# 7Bæ¨¡å‹åœ¨ä¸åŒç²¾åº¦ä¸‹çš„å†…å­˜éœ€æ±‚\n",
        "model_7b_fp32 = 28\n",
        "model_7b_fp16 = 14\n",
        "model_7b_int8 = 7\n",
        "model_7b_int4 = 3.5\n",
        "\n",
        "x_pos = np.arange(len(gpu_memories))\n",
        "width = 0.2\n",
        "\n",
        "ax6.bar(x_pos - 1.5*width, [model_7b_fp32]*4, width, label='FP32', color='red', alpha=0.7)\n",
        "ax6.bar(x_pos - 0.5*width, [model_7b_fp16]*4, width, label='FP16', color='orange', alpha=0.7)\n",
        "ax6.bar(x_pos + 0.5*width, [model_7b_int8]*4, width, label='INT8', color='green', alpha=0.7)\n",
        "ax6.bar(x_pos + 1.5*width, [model_7b_int4]*4, width, label='INT4', color='blue', alpha=0.7)\n",
        "\n",
        "# æ·»åŠ GPUå†…å­˜é™åˆ¶çº¿\n",
        "for i, limit in enumerate(memory_limits):\n",
        "    ax6.axhline(y=limit, xmin=(i-0.4)/len(gpu_memories), xmax=(i+0.4)/len(gpu_memories),\n",
        "                color='black', linestyle='--', linewidth=2)\n",
        "\n",
        "ax6.set_ylabel('å†…å­˜éœ€æ±‚ (GB)')\n",
        "ax6.set_title('7Bæ¨¡å‹åœ¨ä¸åŒGPUä¸Šçš„é€‚é…æ€§')\n",
        "ax6.set_xticks(x_pos)\n",
        "ax6.set_xticklabels(gpu_memories)\n",
        "ax6.legend()\n",
        "ax6.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# æ‰“å°å…³é”®ç»Ÿè®¡ä¿¡æ¯\n",
        "print(\"\\nğŸ“Š é‡åŒ–æ•ˆæœæ€»ç»“:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"å†…å­˜å‹ç¼©æ•ˆæœ:\")\n",
        "print(f\"  8-bité‡åŒ–: {memory_fp32/memory_int8:.1f}x å‹ç¼©\")\n",
        "print(f\"  4-bité‡åŒ–: {memory_fp32/memory_int4:.1f}x å‹ç¼©\")\n",
        "print(f\"\\nç²¾åº¦ä¿æŒ:\")\n",
        "print(f\"  8-bitä¿¡å™ªæ¯”: {snr_8bit:.1f} dB\")\n",
        "print(f\"  4-bitä¿¡å™ªæ¯”: {snr_4bit:.1f} dB\")\n",
        "print(f\"\\nå®é™…åº”ç”¨å»ºè®®:\")\n",
        "print(f\"  8GB GPU: ä½¿ç”¨4-bité‡åŒ– âœ…\")\n",
        "print(f\"  16GB GPU: ä½¿ç”¨8-bité‡åŒ– âœ…\")\n",
        "print(f\"  24GB+ GPU: å¯ä»¥ä½¿ç”¨FP16 âœ…\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. æ€»ç»“ä¸å®é™…åº”ç”¨ ğŸ¯\n",
        "\n",
        "### é‡åŒ–æŠ€æœ¯çš„æ ¸å¿ƒè¦ç‚¹\n",
        "\n",
        "#### ğŸ”¥ é‡åŒ–æœ¬è´¨\n",
        "- **æ ¸å¿ƒåŸç†**: å°†é«˜ç²¾åº¦æµ®ç‚¹æ•°æ˜ å°„åˆ°ä½ç²¾åº¦æ•´æ•°èŒƒå›´\n",
        "- **æ•°å­¦åŸºç¡€**: çº¿æ€§é‡åŒ–å…¬å¼ `quantized = (original - zero_point) / scale`\n",
        "- **åé‡åŒ–**: `reconstructed = quantized * scale + zero_point`\n",
        "\n",
        "#### ğŸ’¾ å†…å­˜èŠ‚çœæ•ˆæœ\n",
        "| ç²¾åº¦ç±»å‹ | æ¯å‚æ•°å­—èŠ‚æ•° | 7Bæ¨¡å‹å†…å­˜éœ€æ±‚ | å‹ç¼©æ¯” |\n",
        "|---------|-------------|---------------|--------|\n",
        "| FP32    | 4 bytes     | 28 GB        | 1x     |\n",
        "| FP16    | 2 bytes     | 14 GB        | 2x     |\n",
        "| INT8    | 1 byte      | 7 GB         | 4x     |\n",
        "| INT4    | 0.5 bytes   | 3.5 GB       | 8x     |\n",
        "\n",
        "#### âš–ï¸ ç²¾åº¦æƒè¡¡\n",
        "- **8-bité‡åŒ–**: è¯¯å·®å¾ˆå°ï¼Œå‡ ä¹æ— æŸ\n",
        "- **4-bité‡åŒ–**: æœ‰ä¸€å®šè¯¯å·®ï¼Œä½†å®é™…åº”ç”¨ä¸­å¯æ¥å—\n",
        "- **è¯¯å·®ç´¯ç§¯**: éšç½‘ç»œæ·±åº¦å¢åŠ ï¼Œä½†å½±å“æœ‰é™\n",
        "\n",
        "### ğŸ› ï¸ å®é™…åº”ç”¨æŒ‡å—\n",
        "\n",
        "#### GPUé€‰æ‹©ç­–ç•¥\n",
        "```python\n",
        "# æ ¹æ®GPUæ˜¾å­˜é€‰æ‹©é‡åŒ–ç­–ç•¥\n",
        "if gpu_memory <= 8:\n",
        "    quantization = \"4-bit\"  # å¿…é¡»\n",
        "elif gpu_memory <= 16:\n",
        "    quantization = \"8-bit\"  # æ¨è\n",
        "else:\n",
        "    quantization = \"FP16\"   # å¯é€‰\n",
        "```\n",
        "\n",
        "#### æ¨¡å‹åŠ è½½æœ€ä½³å®è·µ\n",
        "```python\n",
        "# LLaVAæ¨¡å‹åŠ è½½ç¤ºä¾‹\n",
        "model = load_pretrained_model(\n",
        "    model_path=\"liuhaotian/llava-v1.5-7b\",\n",
        "    load_4bit=True,          # 4-bité‡åŒ–\n",
        "    device_map=\"auto\",       # è‡ªåŠ¨åˆ†é…\n",
        ")\n",
        "```\n",
        "\n",
        "### ğŸš€ æ€§èƒ½ä¼˜åŒ–æŠ€å·§\n",
        "\n",
        "1. **æ™ºèƒ½å†…å­˜ç®¡ç†**: `device_map=\"auto\"` è‡ªåŠ¨åˆ†é…GPU/CPU\n",
        "2. **æ··åˆç²¾åº¦**: å…³é”®å±‚ä½¿ç”¨é«˜ç²¾åº¦ï¼Œå…¶ä»–å±‚é‡åŒ–\n",
        "3. **åŠ¨æ€åŠ è½½**: æŒ‰éœ€åŠ è½½æ¨¡å‹å±‚ï¼Œå‡å°‘æ˜¾å­˜å ç”¨\n",
        "4. **æ¢¯åº¦æ£€æŸ¥ç‚¹**: è®­ç»ƒæ—¶èŠ‚çœæ˜¾å­˜\n",
        "\n",
        "### ğŸ“ å­¦ä¹ æ”¶è·\n",
        "\n",
        "é€šè¿‡æœ¬notebookï¼Œä½ åº”è¯¥ç†è§£äº†ï¼š\n",
        "- âœ… ä¸ºä»€ä¹ˆ8GB GPUéœ€è¦4-bité‡åŒ–\n",
        "- âœ… é‡åŒ–çš„æ•°å­¦åŸç†å’Œå®ç°æ–¹æ³•\n",
        "- âœ… é‡åŒ–å¯¹æ¨¡å‹æ€§èƒ½çš„å®é™…å½±å“\n",
        "- âœ… å¦‚ä½•é€‰æ‹©é€‚åˆçš„é‡åŒ–ç­–ç•¥\n",
        "\n",
        "### ğŸ”— æ‰©å±•é˜…è¯»\n",
        "\n",
        "- [Quantization and Training of Neural Networks](https://arxiv.org/abs/1712.05877)\n",
        "- [LLM.int8(): 8-bit Matrix Multiplication](https://arxiv.org/abs/2208.07339)\n",
        "- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ‰ æ­å–œï¼ä½ å·²ç»æŒæ¡äº†ç¥ç»ç½‘ç»œé‡åŒ–çš„æ ¸å¿ƒæ¦‚å¿µï¼**\n",
        "\n",
        "ç°åœ¨ä½ å¯ä»¥è‡ªä¿¡åœ°åœ¨æœ‰é™çš„ç¡¬ä»¶èµ„æºä¸Šè¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹äº†ï¼ ğŸš€\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
